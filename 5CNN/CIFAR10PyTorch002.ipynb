{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquapathos/BasicAI/blob/master/5CNN%20/CIFAR10PyTorch002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=red>**ã€å…±æœ‰ä¸­ã«ä»˜ãç·¨é›†ä¸å¯ã€‘**ã€€æ”¹å¤‰ã—ãŸã„å ´åˆã¯ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€€PR,IAI</font>"
      ],
      "metadata": {
        "id": "dhj_ih_4LJAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title å­¦ç”Ÿæƒ…å ±å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ \n",
        "#@markdown ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒ ã«ã€å­¦ç±ç•ªå·ã¨æ°åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "#@markdown ä¾‹: 24AB1234å±±ç”° å¤ªéƒ, 24S1234æƒ…å ± äºŒéƒ\n",
        "student_info = '23S0001å±±ç”°å¤ªéƒ' #@param {type: \"string\"}\n",
        "\n",
        "# å­¦ç±ç•ªå·ã¨æ°åã®å…¥åŠ›ãƒ»æŠ½å‡ºã‚’è¡Œã†é–¢æ•°\n",
        "import re\n",
        "\n",
        "# ãƒ•ã‚©ãƒ¼ãƒ ã«å…¥åŠ›ã•ã‚ŒãŸæ–‡å­—åˆ—ã‚’ç›´æ¥ä½¿ç”¨\n",
        "student_info_clean = student_info.strip()\n",
        "\n",
        "# å­¦ç±ç•ªå·ã¨æ°åã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹æ­£è¦è¡¨ç¾\n",
        "pattern = r'^\\d{2,4}\\s*[a-zA-Z]{1,2}\\s*(\\d{4})\\s*(.+)$'\n",
        "\n",
        "# å…¥åŠ›å…¨ä½“ãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã«å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯\n",
        "match = re.fullmatch(pattern, student_info_clean)\n",
        "\n",
        "# å­¦ç±ç•ªå·ã®ç¢ºèªã‚’è¡Œã†é–¢æ•°\n",
        "def check_id_last_digits(student_id_full,need=2,mess=\"ç¢ºèªã®ãŸã‚\"):\n",
        "    while True:\n",
        "        try:\n",
        "            check_id = input(f\"{mess}å­¦ç±ç•ªå·ã®ä¸‹{need}æ¡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„.\")\n",
        "            if check_id == student_id_full[-need:]:\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"{check_id}{student_id_full[-need:]}å…¥åŠ›ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚\\n\")\n",
        "                print(\"ãƒ•ã‚©ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿ãŒé–“é•ã£ã¦ã„ã‚‹å ´åˆã¯è¨‚æ­£ã—ã¦ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
        "        except Exception:\n",
        "            print(\"å…¥åŠ›ãŒä¸æ­£ã§ã™ã€‚\")\n",
        "\n",
        "# ä¸‹ï¼’æ¡ã ã‘ãƒã‚§ãƒƒã‚¯\n",
        "def check2():\n",
        "  check_id_last_digits(student_id_full,need=2,mess=\"æ¬¡ã«é€²ã‚€ã«ã¯\")\n",
        "\n",
        "name, student_id_full = None, None\n",
        "if match:\n",
        "    # ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰å­¦ç±ç•ªå·ã¨æ°åã‚’å–å¾—\n",
        "    student_id_full = match.group(1)\n",
        "    name = match.group(2).strip()\n",
        "\n",
        "    # æ°åãŒç©ºã§ã¯ãªã„ã‹ç¢ºèª\n",
        "    if name:\n",
        "        # æ°åã«æ•°å­—ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯\n",
        "        if any(char.isdigit() for char in name):\n",
        "            print(\"ãƒ•ã‚©ãƒ¼ãƒ ã®å­¦ç±ç•ªå·ã‚’ç¢ºèªã—ï¼Œé–“é•ã„ã‚’ä¿®æ­£ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "        else:\n",
        "            # å­¦ç±ç•ªå·ã®ç¢ºèª\n",
        "            check_pass = check_id_last_digits(student_id_full,need=4)\n",
        "            if check_pass:\n",
        "                print(f\"ã‚ˆã†ã“ãã€{name}ã•ã‚“ï¼ã€€å­¦ç¿’ã‚’å§‹ã‚ã¾ã—ã‚‡ã†\\n\")\n",
        "    else:\n",
        "        print(\"æ°åãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å†åº¦ãƒ•ã‚©ãƒ¼ãƒ ã«å…¥åŠ›ã—ã€ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "else:\n",
        "    print(\"å…¥åŠ›å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚å†åº¦ãƒ•ã‚©ãƒ¼ãƒ ã«å…¥åŠ›ã—ã€ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G5WCwF-rMIJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# å‹•ä½œç’°å¢ƒã®ç¢ºèª\n",
        "\n",
        "import datetime\n",
        "import pytz\n",
        "\n",
        "def æ™‚åˆ»è¡¨ç¤º(phase):\n",
        "    japan_tz = pytz.timezone('Asia/Tokyo')\n",
        "    current_japan_time = datetime.datetime.now(japan_tz)\n",
        "    formatted_time = current_japan_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†%Sç§’')\n",
        "    print(phase,formatted_time)\n",
        "    return formatted_time\n",
        "æ™‚åˆ»è¡¨ç¤º('é–‹å§‹æ™‚åˆ»')\n",
        "\n",
        "!pip -q install deep-translator requests\n",
        "!pip -q install imagehash\n",
        "!pip -q install icrawler deep-translator nltk torch transformers pillow\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "if device.type != 'cpu':\n",
        "  print(torch.cuda.get_device_name())\n",
        "  print(torch.cuda.get_device_capability())"
      ],
      "metadata": {
        "id": "eAVM9-ASRcur",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ç”»åƒã®åé›†\n",
        "ç”»åƒã‚’èªè­˜ã™ã‚‹ãŸã‚ã«ã¯ã€ã¾ãšäº‹å‰ã«ãŸãã•ã‚“ã®äº‹ä¾‹ã‚’AIã«æ©Ÿæ¢°å­¦ç¿’ã•ã›ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€\n",
        "å¤§é‡ã®ç”»åƒã‚’è‡ªåˆ†ã§ç”¨æ„ã™ã‚‹ã®ã¯å¤§å¤‰ãªã®ã§ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§åé›†ã™ã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚\n",
        "\n",
        "ä»¥ä¸‹ã€é‡è¦ãªæŒ‡ç¤ºã¯<font color='blue'>é’å­—</font>ï¼Œç‰¹ã«é‡è¦ãªæŒ‡ç¤ºã¯<font color='red'>èµ¤å­—</font>ã«ã—ã¦ã‚ã‚Šã¾ã™ã®ã§ã€è¦‹è½ã¨ã•ãªã„ã‚ˆã†ã«æ¼”ç¿’ã‚’é€²ã‚ã¦ã„ã£ã¦ãã ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "nvxGm2J1GVZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
        "Google Colaboratory ã¯ä¸€å®šæ™‚é–“çµŒéã™ã‚‹ã¨ä½œæ¥­å†…å®¹ãŒå‰Šé™¤ã•ã‚Œã¦ã—ã¾ã„ã¾ã™ã€‚æ¶ˆã—ãŸããªã„ãƒ‡ãƒ¼ã‚¿ã‚„å¾Œæ—¥å†åˆ©ç”¨ã—ãŸã„ãƒ‡ãƒ¼ã‚¿ã¯ Google Drive ã«ä¿å­˜ã™ã‚‹ã‚ˆã†ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã™ã‚‹ã“ã¨ã§ã€æ®‹ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "\n",
        "\n",
        "1. <font color='blue'>ã“ã®èª¬æ˜ã®ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹\n",
        "2. ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã®ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ãŒå‡ºã‚‹ã€‚ã€ŒGoogleãƒ‰ãƒ©ã‚¤ãƒ–ã«æ¥ç¶šã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã€‚\n",
        "3. ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’é¸æŠã™ã‚‹ã€‚\n",
        "4. ã‚¢ã‚¯ã‚»ã‚¹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã«å¤‰ã‚ã‚‹ã€‚ä¸€ç•ªä¸‹ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€Œè¨±å¯ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã€‚\n",
        "5. å·¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚¨ãƒªã‚¢ã§ã€Œæ›´æ–°ã‚¢ã‚¤ã‚³ãƒ³ã€ï¼ˆå›è»¢ãƒãƒ¼ã‚¯ï¼‰ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼ˆã‚¨ãƒªã‚¢ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ãªã„ãªã‚‰ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¤ã‚³ãƒ³ã€ï¼ˆãƒ•ã‚©ãƒ«ãƒ€å‹ï¼‰ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼‰</font>\n",
        "\n",
        "\n",
        "<img width=\"390\" alt=\"googlecolab\" src=\"https://user-images.githubusercontent.com/5820803/94802343-739cff00-0422-11eb-8c0d-affa919f8e58.png\">\n",
        "\n",
        "ã€€<font color=blue>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„</font>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A3mdVCniM_QT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2tJ9sNVUNHpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. ç”»åƒã®åé›†ã¨é¸åˆ¥\n",
        "\n",
        "è‡ªåˆ†ã§åé›†ã—ãŸç”»åƒã‚’ä½¿ã£ã¦ç‹¬è‡ªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã€å…ˆã¨åŒæ§˜ã®å®Ÿé¨“ã‚’ã‚„ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "## 2-1 ç”»åƒã®åé›†\n",
        "\n",
        "ç”»åƒã‚’åé›†ã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æº–å‚™ã—ã¾ã™ã€‚  <font color='blue'>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„</font>"
      ],
      "metadata": {
        "id": "h4Mdz0p8NYjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [C1.1] ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæº–å‚™\n",
        "# ==============================================================================\n",
        "# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæº–å‚™\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import nltk\n",
        "from deep_translator import GoogleTranslator\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# from icrawler.builtin import GoogleImageCrawler\n",
        "import warnings\n",
        "import datetime\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "ViewSIZE = 128 # è¡¨ç¤ºç”¨ç”»åƒã‚µã‚¤ã‚ºé«˜ã•ã¨å¹…\n",
        "# SIZE = 32 # ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹éš›ã®ç”»åƒã‚µã‚¤ã‚º"
      ],
      "metadata": {
        "id": "4WnO7WkwNHbP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ©Ÿæ¢°å­¦ç¿’ã®éš›ã«ç”»åƒã®ã‚µã‚¤ã‚ºã‚’ã™ã¹ã¦åŒã˜ã«æƒãˆã¾ã™ã€‚ViewSIZEã¯è¡¨ç¤ºç”¨ã®ç”»åƒã‚µã‚¤ã‚ºã€SIZEã¯å­¦ç¿’ãƒ»èªè­˜æ™‚ã®ã‚µã‚¤ã‚ºã§ã™ã€‚  \n",
        "CIFAR10ã«åˆã‚ã›ã‚‹ãŸã‚ã«SIZE = 32ã¨ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "æ¬¡ã«ã€åé›†ã™ã‚‹ç”»åƒã®ã‚«ãƒ†ã‚´ãƒªã‚’æ±ºã‚ã¾ã—ã‚‡ã†ã€‚"
      ],
      "metadata": {
        "id": "q8w-Woc3NtRD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEfwOxDSdIL2"
      },
      "source": [
        "\n",
        "\n",
        "--\n",
        "CIFAR-10 ã®ã‚«ãƒ†ã‚´ãƒªã¯\n",
        "- airplane (é£›è¡Œæ©Ÿ)\n",
        "- automobile (è‡ªå‹•è»Šï¼‰\n",
        "- bird (é³¥é¡ï¼‰\n",
        "- cat ï¼ˆãƒã‚³)\n",
        "- deer (ã‚·ã‚«ï¼‰\n",
        "- dog ï¼ˆã‚¤ãƒŒï¼‰\n",
        "- frog ï¼ˆã‚«ã‚¨ãƒ«ï¼‰\n",
        "- horseï¼ˆã‚¦ãƒï¼‰\n",
        "- shipï¼ˆèˆ¹èˆ¶ï¼‰\n",
        "- truckï¼ˆãƒˆãƒ©ãƒƒã‚¯ï¼‰\n",
        "\n",
        "ã§ã—ãŸã€‚æ¯”è¼ƒã—ãŸã„ã®ã§ã€<font color=red>**ã“ã“ã«å«ã¾ã‚Œã‚‹ã®ä¸­ã‹ã‚‰ã€Œãƒã‚³ã€ã¨ãƒã‚³ä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã‚’ï¼‘ã¤**</font>ã€ã•ã‚‰ã«ã€ã“ã“ã«<font color=red>**å«ã¾ã‚Œãªã„ã‚«ãƒ†ã‚´ãƒªã‚’ï¼’ã¤**</font>ï¼Œè¨ˆï¼”ã¤ã®ã‚«ãƒ†ã‚´ãƒªã‚’è€ƒãˆã¾ã—ã‚‡ã†ã€‚\n",
        "ä»¥ä¸‹ã§ã¯ã€CIFAR10ã®ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ã€Œãƒã‚³ã€ã¨ã€Œã‚¤ãƒŒã€ã€è¿½åŠ ã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦ã€Œç¯å°ã€ã€Œã‚¹ã‚¯ãƒ¼ã‚¿ã€ã‚’é¸ã‚“ã ã‚‚ã®ã¨ã—ã¦èª¬æ˜ã—ã¦ã„ãã¾ã™ã€‚\n",
        "\n",
        "## **<font color=red>ç¢ºèª å®Ÿé¨“ã«ä½¿ã†ï¼”ã¤ã®ã‚«ãƒ†ã‚´ãƒª</font>**\n",
        "ãƒ»ã€Œãƒã‚³ã€ã‚’ï¼”ã¤ã®ã†ã¡ã®ä¸€ã¤ã¨ã—ã¦å¿…ãšå«ã‚ã‚‹ï¼  \n",
        "ãƒ»CIFAR-10ã®10ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ï¼Œãƒã‚³ä»¥å¤–ã«ä¸€ã¤é¸ã¶ï¼  \n",
        "ãƒ»ä¸Šã«å«ã¾ã‚Œãªã„ã‚«ãƒ†ã‚´ãƒªã‚’ï¼’ã¤ï¼Œè¨ˆï¼”ã‚«ãƒ†ã‚´ãƒªã‚’è€ƒãˆã‚‹ï¼"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2]ã®ã‚»ãƒ«ã® myclasses ã®ã‚«ãƒ†ã‚´ãƒªåã‚’è‡ªåˆ†ã§é¸ã‚“ã ï¼”ã‚«ãƒ†ã‚´ãƒªã«æ›¸ãæ›ãˆã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚  \n",
        "\n",
        "  <font color='blue'>ä¸‹ã®ï¼’ã¤ã®ã‚»ãƒ«ã‚’é †ã«å®Ÿè¡Œã—ã¦ãã ã•ã„</font>"
      ],
      "metadata": {
        "id": "W-Uh6b70F6S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [1.2]  ã‚«ãƒ†ã‚´ãƒªæ¡ä»¶ã®ãƒã‚§ãƒƒã‚¯é–¢æ•°\n",
        "# ---------------------------------------------------------------\n",
        "# myclasses ã®æ¤œè¨¼\n",
        "# ---------------------------------------------------------------\n",
        "CIFAR10_CATEGORIES = ['é£›è¡Œæ©Ÿ', 'è‡ªå‹•è»Š', 'é³¥é¡', 'ãƒã‚³', 'ã‚·ã‚«', 'ã‚¤ãƒŒ', 'ã‚«ã‚¨ãƒ«', 'ã‚¦ãƒ', 'èˆ¹èˆ¶', 'ãƒˆãƒ©ãƒƒã‚¯']\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# æ¤œè¨¼ç”¨é–¢æ•°ã®å®šç¾© (æ±ç”¨ç‰ˆ)\n",
        "# ---------------------------------------------------------------\n",
        "def validate_myclasses(categories, required_category='ãƒã‚³', dataset_categories=CIFAR10_CATEGORIES, expected_size=4):\n",
        "    \"\"\"myclassesã®æ¡ä»¶ã‚’æŸ”è»Ÿã«ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "    errors = []\n",
        "    dataset_categories = list(dataset_categories) if dataset_categories else []\n",
        "\n",
        "    if len(categories) != expected_size:\n",
        "        errors.append(f\"âŒ ã‚«ãƒ†ã‚´ãƒªæ•°ãŒ{len(categories)}å€‹ã§ã™ã€‚{expected_size}å€‹ã«ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "    if required_category and required_category not in categories:\n",
        "        errors.append(f\"âŒ ã€Œ{required_category}ã€ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "    dataset_except_required = [c for c in dataset_categories if c != required_category]\n",
        "    dataset_in_myclasses = [c for c in categories if c in dataset_except_required]\n",
        "\n",
        "    if len(dataset_in_myclasses) == 0:\n",
        "        selectable = ', '.join(dataset_except_required) if dataset_except_required else '(ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœªæŒ‡å®š)'\n",
        "        errors.append(f\"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚«ãƒ†ã‚´ãƒªï¼ˆ{required_category}ä»¥å¤–ï¼‰ãŒ1ã¤ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\\n   æ¬¡ã‹ã‚‰1ã¤é¸ã‚“ã§ãã ã•ã„: {selectable}\")\n",
        "    elif len(dataset_in_myclasses) > 1:\n",
        "        errors.append(f\"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚«ãƒ†ã‚´ãƒªï¼ˆ{required_category}ä»¥å¤–ï¼‰ãŒ{len(dataset_in_myclasses)}å€‹å«ã¾ã‚Œã¦ã„ã¾ã™ï¼ˆ{', '.join(dataset_in_myclasses)}ï¼‰ã€‚\\n   1å€‹ã ã‘ã«ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "    non_required_categories = [c for c in categories if c != required_category]\n",
        "    non_dataset_categories = [c for c in non_required_categories if c not in dataset_categories]\n",
        "    required_original_count = max(expected_size - 2, 0)\n",
        "\n",
        "    if len(non_dataset_categories) < required_original_count:\n",
        "        errors.append(\n",
        "            f\"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œãªã„ç‹¬è‡ªã®ã‚«ãƒ†ã‚´ãƒªãŒ{len(non_dataset_categories)}å€‹ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚\\n   {required_original_count}å€‹å¿…è¦ã§ã™ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {', '.join(dataset_categories)}ï¼‰\"\n",
        "        )\n",
        "\n",
        "    if errors:\n",
        "        print(\"\\nâš ï¸  ä»¥ä¸‹ã®å•é¡ŒãŒã‚ã‚Šã¾ã™:\\n\")\n",
        "        for error in errors:\n",
        "            print(error)\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸ”§ ä¿®æ­£ä¾‹:\")\n",
        "        example_dataset = dataset_except_required[0] if dataset_except_required else 'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ†ã‚´ãƒª'\n",
        "        example_originals = [f\"ç‹¬è‡ªã‚«ãƒ†ã‚´ãƒª{i+1}\" for i in range(required_original_count)]\n",
        "        example = [required_category if required_category else 'å¿…é ˆã‚«ãƒ†ã‚´ãƒª', example_dataset] + example_originals\n",
        "        example = example[:expected_size]\n",
        "        print(f\"myclasses = {example}\")\n",
        "        raise ValueError(\"myclasses ã®è¨­å®šãŒæ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“ã€‚ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "    else:\n",
        "        dataset_items = [c for c in categories if c in dataset_except_required]\n",
        "        original_items = non_dataset_categories\n",
        "        print(\"\\nâœ… ã™ã¹ã¦ã®æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™ï¼\")\n",
        "        if required_category:\n",
        "            print(f\"   - å¿…é ˆ: {required_category}\")\n",
        "        print(f\"   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_items[0] if dataset_items else '(ãªã—)'}\")\n",
        "        print(f\"   - ç‹¬è‡ªã‚«ãƒ†ã‚´ãƒª: {', '.join(original_items) if original_items else '(ãªã—)'}\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "print(\"ä¸Šã®èª¬æ˜ã‚’èª­ã¾ãšã«æ¬¡ã‚’å®Ÿè¡Œã—ã¦ã—ã¾ã‚ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®é–¢æ‰€ã§ã™ï¼ \\nä¸Šã®æŒ‡ç¤ºé€šã‚Šã€Œç¢ºèªã€è¨˜è¼‰é€šã‚Šã«ã‚«ãƒ†ã‚´ãƒªã‚’é¸ã³ï¼Œä¸‹ã®ã‚»ãƒ«ã®myclassesã‚’æ›¸ãæ›ãˆã¦ã‹ã‚‰å…ˆã«é€²ã‚“ã§ãã ã•ã„ã„ï¼\")\n",
        "check2()"
      ],
      "metadata": {
        "id": "o8kyDY_8M2N-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  [ï¼’]  è¨­å®š & åé›†ã‚«ãƒ†ã‚´ãƒªå®šç¾©\n",
        "# ==============================================================================\n",
        "# 2. è¨­å®š & AIã‚¯ãƒ©ã‚¹å®šç¾©\n",
        "# ==============================================================================\n",
        "# åé›†ã—ãŸã„ã‚«ãƒ†ã‚´ãƒª  ï¼ˆæ¬¡ã®è¡Œã¯ä¾‹ã§ã™ã€‚'ãƒã‚³'ä»¥å¤–ã‚’æ›¸ãæ›ãˆã¦ãã ã•ã„ï¼‰\n",
        "myclasses = ['ãƒã‚³','ã‚¤ãƒŒ','ç¯å°','ã‚¹ã‚¯ãƒ¼ã‚¿']\n",
        "\n",
        "# ä¿å­˜å…ˆè¨­å®š (â˜…å¤‰æ›´: Driveã§ã¯ãªããƒ­ãƒ¼ã‚«ãƒ«ã«å¤‰æ›´)\n",
        "target_num = 300\n",
        "TMPFOLDER = './tmp_images/'   # Colabå†…éƒ¨ã®é«˜é€Ÿã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ä½¿ç”¨\n",
        "\n",
        "# æœ€çµ‚çš„ãªä¿å­˜å…ˆ (Google Drive)\n",
        "FINAL_SAVE_DIR = 'drive/MyDrive/tmp/'\n",
        "\n",
        "# ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š\n",
        "USE_CLIP_FILTER = True\n",
        "CLIP_THRESHOLD = 0.22   # 0.20ã€œ0.25æ¨å¥¨\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# æ¤œè¨¼å®Ÿè¡Œ\n",
        "# ---------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“‹ myclasses ã®æ¤œè¨¼\")\n",
        "print(\"=\"*60)\n",
        "print(f\"è¨­å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒª: {myclasses}\")\n",
        "print(f\"ä½œæ¥­å ´æ‰€ (é«˜é€Ÿ): {TMPFOLDER}\")\n",
        "print(f\"ä¿å­˜å ´æ‰€ (Drive): {FINAL_SAVE_DIR}\")\n",
        "\n",
        "validate_myclasses(myclasses, required_category='ãƒã‚³', dataset_categories=CIFAR10_CATEGORIES, expected_size=4)"
      ],
      "metadata": {
        "id": "Qt0hYq6mEooS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ãƒãƒƒãƒˆä¸Šã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ç”»åƒã‚’åé›†ã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€icrawler ã‚’ä½¿ã£ã¦ç”»åƒã‚’åé›†ã—ã¾ã™ã€‚ ä»Šå›ã¯ Bingã‹ã‚‰ç”»åƒã‚’æ‹¾ã£ã¦ãã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚ï¼ˆä»¥å‰ã¯Googleæ¤œç´¢ã‚’ä½¿ã£ã¦ã„ã¾ã—ãŸãŒï¼Œä»•æ§˜å¤‰æ›´ã§ä½¿ãˆãªããªã‚Šã¾ã—ãŸï¼ï¼‰\n",
        "\n",
        "æ¬¡ã®ã‚»ãƒ«ã¯ã€ä¸Šã§é¸ã‚“ã ã‚«ãƒ†ã‚´ãƒªã®ç”»åƒã‚’ãã‚Œãã‚Œï¼“ï¼ï¼æšã‚’ç›®æ¨™ã¨ã—ã¦åé›†ã—ã€ä¸€æ—¦ Google Drive ã®ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–å†…ã® 'tmp'ã¨ã„ã†åã®ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ã™ã€‚tmpãƒ•ã‚©ãƒ«ãƒ€å†…ã«ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ãŒã§ãã¾ã™ã€‚ï¼ˆ \"drive/MyDrive\" ãŒ Google Driveã®ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–ã‚’è¡¨ã—ã¦ã„ã¾ã™ï¼‰\n",
        "\n",
        "  <font color='blue'>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„</font> ï¼ˆå®šç¾©ã ã‘ãªã®ã§ä½•ã‚‚èµ·ã“ã‚Šã¾ã›ã‚“ï¼‰"
      ],
      "metadata": {
        "id": "WvGuibj9Ysjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ç”»åƒåé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®šç¾©\n",
        "# ---------------------------------------------------------------\n",
        "# åˆæœŸæº–å‚™\n",
        "# ---------------------------------------------------------------\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"âœ… å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹: {DEVICE}\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# ImageFilterAI ã‚¯ãƒ©ã‚¹å®šç¾© (é«˜é€ŸåŒ–+ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ç‰ˆ)\n",
        "# ---------------------------------------------------------------\n",
        "# é¡”æ¤œå‡ºå™¨ã®æº–å‚™\n",
        "haar_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "face_cascade = cv2.CascadeClassifier(haar_path)\n",
        "\n",
        "class ImageFilterAI:\n",
        "    def __init__(self):\n",
        "        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã®DEVICEå¤‰æ•°ãŒãªãã¦ã‚‚å‹•ãã‚ˆã†ã€ã“ã“ã§ã‚‚ãƒã‚§ãƒƒã‚¯\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "            self.batch_size = 24\n",
        "            self.dtype = torch.float16\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "            self.batch_size = 4\n",
        "            self.dtype = torch.float32\n",
        "\n",
        "        print(f\"   ğŸš€ AI Filter Device: {self.device}\")\n",
        "\n",
        "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device, dtype=self.dtype)\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.translator = GoogleTranslator(source='auto', target='en')\n",
        "        self.face_cascade = face_cascade\n",
        "\n",
        "    def _remove_duplicates(self, folder_path):\n",
        "        \"\"\"ãƒ•ã‚©ãƒ«ãƒ€å†…ã®é‡è¤‡ç”»åƒã‚’imagehashã§æ¤œå‡ºãƒ»å‰Šé™¤\"\"\"\n",
        "        import imagehash\n",
        "        from PIL import Image\n",
        "\n",
        "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
        "        if not image_files:\n",
        "            return 0\n",
        "\n",
        "        hashes = {}\n",
        "        removed = 0\n",
        "\n",
        "        for fname in tqdm(image_files, desc=\"Duplicate Check\", unit=\"img\", leave=False):\n",
        "            path = os.path.join(folder_path, fname)\n",
        "            try:\n",
        "                with Image.open(path) as img:\n",
        "                    hash_val = imagehash.phash(img)\n",
        "                    if hash_val in hashes:\n",
        "                        os.remove(path)\n",
        "                        removed += 1\n",
        "                    else:\n",
        "                        hashes[hash_val] = fname\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    def _has_human(self, image_path):\n",
        "        try:\n",
        "            img = cv2.imread(image_path)\n",
        "            if img is None: return False\n",
        "            height, width = img.shape[:2]\n",
        "            if width > 1000:\n",
        "                scale = 800 / width\n",
        "                img = cv2.resize(img, None, fx=scale, fy=scale)\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "            return len(faces) > 0\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _prepare_labels(self, category_text, negative_words):\n",
        "        pos = self.translator.translate(category_text)\n",
        "        negs = [self.translator.translate(n) for n in negative_words]\n",
        "        pos_labels = [f\"{pos}\", f\"a {pos}\", f\"a real photo of a {pos}\"]\n",
        "        neg_labels = [f\"a {n}\" for n in negs] + [\"text\", \"watermark\", \"illustration\", \"cartoon\"]\n",
        "        return pos_labels + neg_labels\n",
        "\n",
        "    def filter(self, folder_path, category_text, negative_words, threshold, is_human_category=False):\n",
        "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
        "        if not image_files: return\n",
        "\n",
        "        labels = self._prepare_labels(category_text, negative_words)\n",
        "        deleted = 0\n",
        "        human_filtered = 0\n",
        "\n",
        "        if not is_human_category:\n",
        "            print(f\"   ğŸ‘¤ äººç‰©æ¤œå‡ºãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ä¸­...\")\n",
        "            files_to_process = []\n",
        "            for fname in tqdm(image_files, desc=\"Checking Faces\", unit=\"img\", leave=False):\n",
        "                path = os.path.join(folder_path, fname)\n",
        "                if self._has_human(path):\n",
        "                    try: os.remove(path); human_filtered += 1\n",
        "                    except: pass\n",
        "                else:\n",
        "                    files_to_process.append(fname)\n",
        "            print(f\"      âœ… äººç‰©å‰Šé™¤å®Œäº†: {human_filtered} æšå‰Šé™¤ã—ã¾ã—ãŸ\")\n",
        "            image_files = files_to_process\n",
        "\n",
        "        if not image_files: return\n",
        "\n",
        "        print(f\"   ğŸ¤– CLIPæ„å‘³ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ä¸­...\")\n",
        "        batch_indices = range(0, len(image_files), self.batch_size)\n",
        "        for i in tqdm(batch_indices, desc=\"AI Filtering\", unit=\"batch\", leave=False):\n",
        "            batch_files = image_files[i:i+self.batch_size]\n",
        "            images = []\n",
        "            valid_batch_files = []\n",
        "            for fname in batch_files:\n",
        "                path = os.path.join(folder_path, fname)\n",
        "                try:\n",
        "                    with Image.open(path) as img:\n",
        "                        images.append(img.convert(\"RGB\"))\n",
        "                        valid_batch_files.append(fname)\n",
        "                except:\n",
        "                    try: os.remove(path); deleted += 1\n",
        "                    except: pass\n",
        "            if not images: continue\n",
        "\n",
        "            try:\n",
        "                inputs = self.processor(text=labels, images=images, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "                if self.device == \"cuda\":\n",
        "                    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(dtype=self.dtype)\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(**inputs)\n",
        "                    probs = outputs.logits_per_image.softmax(dim=1)\n",
        "                pos_count = len(labels) // 2\n",
        "                for fname, p in zip(valid_batch_files, probs):\n",
        "                    path = os.path.join(folder_path, fname)\n",
        "                    pos_score = p[:pos_count].max().item()\n",
        "                    best_idx = p.argmax().item()\n",
        "                    if best_idx >= pos_count or pos_score < threshold:\n",
        "                        try: os.remove(path); deleted += 1\n",
        "                        except: pass\n",
        "            except Exception:\n",
        "                continue\n",
        "        # é‡è¤‡é™¤å»\n",
        "        print(f\"   ğŸ” é‡è¤‡ç”»åƒæ¤œå‡ºä¸­...\")\n",
        "        duplicates = self._remove_duplicates(folder_path)\n",
        "        if duplicates is None:\n",
        "            duplicates = 0\n",
        "        if duplicates > 0:\n",
        "            print(f\"      âœ… é‡è¤‡å‰Šé™¤å®Œäº†: {duplicates} æšå‰Šé™¤\")\n",
        "\n",
        "        print(f\"      âœ… CLIPé¸åˆ¥å®Œäº†: {deleted} æšå‰Šé™¤ï¼ˆåˆè¨ˆå‰Šé™¤: {deleted + human_filtered + duplicates} æšï¼‰\")\n",
        "\n",
        "æ™‚åˆ»è¡¨ç¤º('CLIPãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†')\n",
        "# ---------------------------------------------------------------\n",
        "# UI/ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³éƒ¨åˆ†ã¯ç›´æ¥ã‚»ãƒ«ã§ input() ã‚’ä½¿ã†å‰æ\n",
        "# - search_keyword_ui(), process_global_queue() ã¯å‰Šé™¤æ¸ˆã¿\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# GLOBAL_SEARCH_QUEUE = {}\n",
        "print(\"âœ… æº–å‚™å®Œäº†ã€‚æ¬¡ã®ã‚»ãƒ«ã§ UI ã‚„åé›†å‡¦ç†ã‚’å®Ÿè¡Œã§ãã¾ã™\")"
      ],
      "metadata": {
        "id": "6UPwd8c3Y3rF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [3] æ¤œç´¢èªã®æ±ºå®š\n",
        "\n",
        "æ¤œç´¢ã—ãŸã„ã‚‚ã®ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ’é™¤ã—ãŸã„ã‚‚ã®ã‚’è¡¨ã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ±ºã‚ã¾ã™ã€‚\n",
        "\n",
        "## æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¿®æ­£\n",
        "æ¤œç´¢ã¯è‹±èªã‚’ä½¿ã£ãŸæ–¹ãŒå¤šããƒ’ãƒƒãƒˆã™ã‚‹ã®ã§ã€ã¾ãšæ—¥æœ¬èªã‚’è‹±èªã«å¤‰æ›ã—ã€é¡ç¾©èªã‚’AIã«ææ¡ˆã•ã›ã¾ã™ã€‚\n",
        "\n",
        "èªã®ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ãŒæ—¥æœ¬èªã¨è‹±èªã¨ã§ãšã‚Œã¦ã„ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€ã€Œã‚¹ã‚¯ãƒ¼ã‚¿ã€ã¯è‹±èªã§ã¯ã€Œscooterã€ã§ã™ãŒã€è‹±èªã®ã€Œscooterã€ã«ã¯æµ·ä¸Šã‚¹ã‚¯ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¾ã™ã€‚\n",
        "\n",
        "ã‚¹ã‚¯ãƒ¼ã‚¿ã€€â†’ã€€scooterã€€â†’ sea scooter, motor scooter, water scooter\n",
        "\n",
        "AIãŒæ¤œç´¢èªã®å€™è£œã‚’ææ¡ˆã—ã¦ãã¾ã™ã€‚ä¸è¦ãªèªãŒã‚ã‚Œã°å‰Šé™¤ã—ã¦ä¸Šæ›¸ãã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "â†’ã€€ã‚¹ã‚¯ãƒ¼ã‚¿, scooter, motor scooter\n",
        "\n",
        "ãã®ã¾ã¾æ¡ç”¨ã™ã‚Œã°è‰¯ã„æ™‚ã¯ç©ºæ¬„ã®ã¾ã¾Enterã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "## æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¿½åŠ \n",
        "AIã®ææ¡ˆã«æ¤œç´¢èªã‚’åŠ ãˆãŸã„å ´åˆã¯ã€+ã«ç¶šã‘ã¦ä»˜ã‘åŠ ãˆãŸã„èªã‚’ä¸¦ã¹ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "+ãƒ›ãƒ³ãƒ€ã‚¹ã‚¯ãƒ¼ã‚¿, Yamaha scooter.\n",
        " â†’ã€€ã‚¹ã‚¯ãƒ¼ã‚¿, scooter, motor scooter, ãƒ›ãƒ³ãƒ€ã‚¹ã‚¯ãƒ¼ã‚¿, Yamaha scooter\n",
        "\n",
        "## æ’é™¤ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\n",
        "\n",
        "æ¤œç´¢ç”»åƒã«ã¯æœ›ã¾ã—ããªã„ã‚‚ã®ã‚‚ãŸãã•ã‚“å«ã¾ã‚Œã¾ã™ã®ã§ã€ã“ã‚Œã¯å«ã‚“ã§ã»ã—ããªã„ã¨ã„ã†ã‚‚ã®ã‚’èã„ã¦ãã¾ã™ã®ã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚¤ãƒ©ã‚¹ãƒˆã‚„ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–ãƒªã‚¹ãƒˆã¨ãªã‚‹ã®ã§ã€+ã«ç¶šã‘ã¦æ’é™¤ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¸¦ã¹ã¦ç­”ãˆã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ä¾‹ï¼‰+çŒ«ãƒã‚¹ã€ã‚­ãƒ£ãƒƒãƒˆã‚¦ã‚©ãƒ¼ã‚¯"
      ],
      "metadata": {
        "id": "27pMGhSTNOtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ---------------------------------------------------------------\n",
        "# StrategyPlanner (å‹•çš„ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒªã‚¹ãƒˆ + æ–‡è„ˆèªè­˜)\n",
        "# ---------------------------------------------------------------\n",
        "# å¿…è¦ãªãƒ¢ãƒ‡ãƒ«æº–å‚™\n",
        "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=torch.device('cpu')) # è»½é‡ãªã®ã§CPUã§OK\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "ACTIONS_AND_CONTEXTS = ['photo', 'close up', 'side view', 'on white background', 'in context', 'in use']\n",
        "HUMAN_KEYWORDS = ['person', 'people', 'human', 'man', 'woman', 'child', 'baby', 'kid','boy', 'girl', 'adult', 'teenager', 'infant', 'toddler','äºº', 'äººé–“', 'ç”·æ€§', 'å¥³æ€§', 'å­ä¾›', 'èµ¤ã¡ã‚ƒã‚“', 'å¤§äºº', 'å°‘å¹´', 'å°‘å¥³']\n",
        "\n",
        "class StrategyPlanner:\n",
        "    def __init__(self, similarity_threshold=0.6, max_candidates=40):\n",
        "        self.to_en = translator\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.max_candidates = max_candidates\n",
        "\n",
        "        # 1. ã€å¸¸ã«ã€‘é™¤å¤–ã—ãŸã„ã‚‚ã®\n",
        "        self.basic_negatives = [\n",
        "            'toy', 'cartoon', 'illustration', 'sketch', 'drawing', 'vector',\n",
        "            'logo', 'text', 'font', 'watermark', 'symbol', 'icon', 'clipart',\n",
        "            'screenshot', '3d render', 'origami', 'miniature', 'model',\n",
        "            'plastic', 'sticker', 'decals'\n",
        "        ]\n",
        "\n",
        "        # 2. ã€é£Ÿã¹ç‰©ä»¥å¤–ã®å ´åˆã®ã¿ã€‘é™¤å¤–ã—ãŸã„ã‚‚ã®\n",
        "        self.food_negatives = [\n",
        "            'food', 'dish', 'recipe', 'cooking', 'meal', 'ingredient', 'cuisine'\n",
        "        ]\n",
        "\n",
        "    def _clip_text_sim(self, base, cands):\n",
        "        if not cands: return []\n",
        "        base_emb = embedder.encode(base, convert_to_tensor=True)\n",
        "        cand_embs = embedder.encode(cands, convert_to_tensor=True)\n",
        "        sims = util.cos_sim(base_emb, cand_embs)[0].cpu().numpy()\n",
        "        return sims.tolist()\n",
        "\n",
        "    def _is_human_category(self, category_jp, category_en):\n",
        "        cat_jp_lower = category_jp.lower()\n",
        "        cat_en_lower = category_en.lower()\n",
        "        for keyword in HUMAN_KEYWORDS:\n",
        "            if keyword in cat_jp_lower or keyword in cat_en_lower:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _is_food_category(self, category_en):\n",
        "        # Aãƒãƒ¼ãƒ : é£Ÿã¹ç‰©\n",
        "        food_anchors = [\"food\", \"dish\", \"meal\", \"cuisine\", \"beverage\", \"dessert\"]\n",
        "        # Bãƒãƒ¼ãƒ : ãã‚Œä»¥å¤–\n",
        "        non_food_anchors = [\n",
        "            \"animal\", \"mammal\", \"bird\", \"fish\", \"insect\", \"pet\",\n",
        "            \"vehicle\", \"car\", \"bicycle\", \"boat\", \"aircraft\",\n",
        "            \"architecture\", \"building\", \"place\", \"scenery\",\n",
        "            \"technology\", \"tool\", \"device\", \"furniture\", \"clothing\"\n",
        "        ]\n",
        "\n",
        "        food_score = max(self._clip_text_sim(category_en, food_anchors) + [0])\n",
        "        non_food_score = max(self._clip_text_sim(category_en, non_food_anchors) + [0])\n",
        "        print(f\"   âš–ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š: Food({food_score:.2f}) vs Non-Food({non_food_score:.2f})\")\n",
        "\n",
        "        if (food_score > non_food_score) and (food_score > 0.3):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _is_bad_word(self, word, active_negatives):\n",
        "        padded_word = f\" {word.lower()} \"\n",
        "        for bad in active_negatives:\n",
        "            if f\" {bad} \" in padded_word:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _fetch_from_conceptnet(self, word_en):\n",
        "        cands = set()\n",
        "        url = f\"http://api.conceptnet.io/query?rel=/r/IsA&end=/c/en/{word_en}&limit=100\"\n",
        "        try:\n",
        "            time.sleep(1.0)\n",
        "            response = requests.get(url, timeout=3.0)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for edge in data['edges']:\n",
        "                    start_label = edge['start']['label'].lower()\n",
        "                    if edge['start']['language'] != 'en' or len(start_label.split()) > 3:\n",
        "                        continue\n",
        "                    cands.add(start_label)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return list(cands)\n",
        "\n",
        "    def _get_recursive_hyponyms(self, synset, depth=0, max_depth=2):\n",
        "        results = set()\n",
        "        if depth >= max_depth: return results\n",
        "        for hypo in synset.hyponyms():\n",
        "            for lemma in hypo.lemmas():\n",
        "                name = lemma.name().replace('_', ' ').lower()\n",
        "                results.add(name)\n",
        "            results.update(self._get_recursive_hyponyms(hypo, depth + 1, max_depth))\n",
        "        for inst in synset.instance_hyponyms():\n",
        "            for lemma in inst.lemmas():\n",
        "                name = lemma.name().replace('_', ' ').lower()\n",
        "                results.add(name)\n",
        "        return results\n",
        "\n",
        "    def _fetch_from_wordnet(self, word_en):\n",
        "        cands = set()\n",
        "        synsets = wordnet.synsets(word_en, pos=wordnet.NOUN)\n",
        "        for syn in synsets[:3]:\n",
        "            found_words = self._get_recursive_hyponyms(syn, depth=0, max_depth=2)\n",
        "            cands.update(found_words)\n",
        "        return list(cands)\n",
        "\n",
        "    def get_subcategories(self, category_jp):\n",
        "        # 1. ç¿»è¨³\n",
        "        try:\n",
        "            cat_en = self.to_en.translate(category_jp).lower()\n",
        "        except:\n",
        "            cat_en = category_jp\n",
        "\n",
        "        print(f\"ğŸ” '{category_jp}' ({cat_en}) ã®é–¢é€£èªã‚’åé›†ä¸­...\")\n",
        "\n",
        "        is_human = self._is_human_category(category_jp, cat_en)\n",
        "        is_food = self._is_food_category(cat_en)\n",
        "\n",
        "        # å‹•çš„ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒªã‚¹ãƒˆã®æ§‹ç¯‰\n",
        "        active_negatives = self.basic_negatives.copy()\n",
        "        if not is_food:\n",
        "            active_negatives.extend(self.food_negatives)\n",
        "            print(\"   ğŸ›¡ï¸ éé£Ÿå“ãƒ¢ãƒ¼ãƒ‰: food, dish ãªã©ã‚’é™¤å¤–ã—ã¾ã™\")\n",
        "        else:\n",
        "            print(\"   ğŸ½ï¸ é£Ÿå“ãƒ¢ãƒ¼ãƒ‰: food, dish ãªã©ã‚’è¨±å®¹ã—ã¾ã™\")\n",
        "\n",
        "        # 2. å€™è£œåé›†\n",
        "        candidates = self._fetch_from_conceptnet(cat_en)\n",
        "        wn_candidates = self._fetch_from_wordnet(cat_en)\n",
        "        candidates.extend(wn_candidates)\n",
        "\n",
        "        candidates = list(set(candidates))\n",
        "        if cat_en not in candidates: candidates.append(cat_en)\n",
        "\n",
        "        # 3. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "        THRESHOLD_SCORE = 0.25\n",
        "        print(f\"   ğŸ¤– {len(candidates)} å€‹ã®å€™è£œã‚’AIè©•ä¾¡ä¸­ (é–¾å€¤: {THRESHOLD_SCORE})...\")\n",
        "\n",
        "        sims = self._clip_text_sim(cat_en, candidates)\n",
        "\n",
        "        valid_candidates = []\n",
        "        for word, score in zip(candidates, sims):\n",
        "            if word == cat_en:\n",
        "                valid_candidates.append((word, 1.0))\n",
        "                continue\n",
        "            if score < THRESHOLD_SCORE:\n",
        "                continue\n",
        "            if self._is_bad_word(word, active_negatives):\n",
        "                continue\n",
        "            valid_candidates.append((word, score))\n",
        "\n",
        "        valid_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        final_list = [w for w, s in valid_candidates[:self.max_candidates]]\n",
        "\n",
        "        if cat_en in final_list: final_list.remove(cat_en)\n",
        "        final_list.insert(0, cat_en)\n",
        "        if category_jp in final_list: final_list.remove(category_jp)\n",
        "        final_list.insert(0, category_jp)\n",
        "\n",
        "        final_list = list(dict.fromkeys(final_list))\n",
        "\n",
        "        print(f\"   -> æ¡ç”¨: {final_list[:10]} ... è¨ˆ {len(final_list)} å€‹\")\n",
        "        return final_list, cat_en, is_human\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³é–¢æ•°ï¼ˆæ”¹è‰¯ç‰ˆ: ç•ªå·æŒ‡å®šã§å‰Šé™¤å¯èƒ½ãªæ–¹å¼ï¼‰\n",
        "# ---------------------------------------------------------\n",
        "def safe_input_loop(prompt_text, default_value, is_additive_mode=False):\n",
        "    \"\"\"\n",
        "    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå…¥åŠ›ãƒ«ãƒ¼ãƒ—\n",
        "\n",
        "    Args:\n",
        "        prompt_text: èª¬æ˜æ–‡\n",
        "        default_value: AIææ¡ˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆãƒªã‚¹ãƒˆã¾ãŸã¯æ–‡å­—åˆ—ï¼‰\n",
        "        is_additive_mode: Trueã®å ´åˆã€å…¥åŠ›ã¯å¸¸ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«è¿½åŠ ã•ã‚Œã‚‹ï¼ˆæ’é™¤èªç”¨ï¼‰\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        print(\"-\" * 40)\n",
        "        print(prompt_text)\n",
        "\n",
        "        if isinstance(default_value, list):\n",
        "            current_list = default_value.copy()\n",
        "        else:\n",
        "            current_list = [default_value]\n",
        "\n",
        "        # ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§è¡¨ç¤º\n",
        "        print(f\"ğŸ‘‰ AIææ¡ˆ:\")\n",
        "        for idx, word in enumerate(current_list, 1):\n",
        "            print(f\"   {idx}. {word}\")\n",
        "\n",
        "        if is_additive_mode:\n",
        "            print(\"\\n   æ“ä½œ:\")\n",
        "            print(\"   - ãã®ã¾ã¾Enterã§ææ¡ˆã‚’æ¡ç”¨\")\n",
        "            print(\"   - è¿½åŠ ã—ãŸã„èªã‚’å…¥åŠ›ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰\")\n",
        "            user_in = input(\">>> \").strip()\n",
        "\n",
        "            if not user_in:\n",
        "                actual_val = current_list\n",
        "                mode_msg = \"ã€AIææ¡ˆã‚’ãã®ã¾ã¾æ¡ç”¨ã€‘\"\n",
        "            else:\n",
        "                add_words = [w.strip().lstrip('+').lstrip('ï¼‹') for w in user_in.replace('ã€', ',').replace('ï¼Œ', ',').split(',') if w.strip()]\n",
        "                actual_val = current_list + add_words\n",
        "                actual_val = list(dict.fromkeys(actual_val))\n",
        "                mode_msg = f\"ã€AIææ¡ˆã«è¿½åŠ ã€‘ (+ {','.join(add_words)})\"\n",
        "        else:\n",
        "            print(\"\\n   æ“ä½œ:\")\n",
        "            print(\"   - ãã®ã¾ã¾Enterã§ææ¡ˆã‚’æ¡ç”¨\")\n",
        "            print(\"   - å‰Šé™¤ã—ãŸã„ç•ªå·ã‚’å…¥åŠ›ï¼ˆä¾‹: 2,4,5ï¼‰\")\n",
        "            print(\"   - è¿½åŠ ã—ãŸã„èªã‚’å…¥åŠ›ï¼ˆä¾‹: +æŸ´çŠ¬,ãƒãƒ¯ãƒ¯ï¼‰\")\n",
        "            print(\"   - å…¨ã¦æ›¸ãæ›ãˆï¼ˆä¾‹: =æŸ´çŠ¬,ãƒãƒ¯ãƒ¯,ç§‹ç”°çŠ¬ï¼‰\")\n",
        "            user_in = input(\">>> \").strip()\n",
        "\n",
        "            if not user_in:\n",
        "                actual_val = current_list\n",
        "                mode_msg = \"ã€AIææ¡ˆã‚’ãã®ã¾ã¾æ¡ç”¨ã€‘\"\n",
        "            elif user_in.startswith('-'):\n",
        "                # å‰Šé™¤ãƒ¢ãƒ¼ãƒ‰: -2,4,5 å½¢å¼\n",
        "                try:\n",
        "                    nums_str = user_in[1:].replace(' ', '')\n",
        "                    nums = [int(n) for n in nums_str.split(',') if n]\n",
        "                    actual_val = [w for i, w in enumerate(current_list, 1) if i not in nums]\n",
        "                    mode_msg = f\"ã€ç•ªå· {','.join(map(str, nums))} ã‚’å‰Šé™¤ã€‘\"\n",
        "                except:\n",
        "                    print(\"âš ï¸ ç•ªå·ã®æŒ‡å®šãŒä¸æ­£ã§ã™ã€‚å†å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\")\n",
        "                    continue\n",
        "            elif user_in.startswith('+') or user_in.startswith('ï¼‹'):\n",
        "                # è¿½åŠ ãƒ¢ãƒ¼ãƒ‰ï¼ˆå…¨è§’ï¼‹ã«ã‚‚å¯¾å¿œï¼‰\n",
        "                start_char = user_in[0]\n",
        "                add_words = [w.strip().lstrip('+').lstrip('ï¼‹') for w in user_in[1:].replace('ã€', ',').replace('ï¼Œ', ',').split(',') if w.strip()]\n",
        "                actual_val = current_list + add_words\n",
        "                actual_val = list(dict.fromkeys(actual_val))\n",
        "                mode_msg = f\"ã€AIææ¡ˆã«è¿½åŠ ã€‘ (+ {','.join(add_words)})\"\n",
        "            elif user_in.startswith('=') or user_in.startswith('ï¼'):\n",
        "                # å…¨ç½®æ›ãƒ¢ãƒ¼ãƒ‰ï¼ˆå…¨è§’ï¼ã«ã‚‚å¯¾å¿œï¼‰\n",
        "                actual_val = [w.strip().lstrip('+').lstrip('ï¼‹') for w in user_in[1:].replace('ã€', ',').replace('ï¼Œ', ',').split(',') if w.strip()]\n",
        "                mode_msg = \"ã€å…¨ã¦æ›¸ãæ›ãˆã€‘\"\n",
        "            elif all(c.isdigit() or c in ',ã€ï¼Œ ' for c in user_in):\n",
        "                # ç•ªå·ã®ã¿å…¥åŠ›ã•ã‚ŒãŸå ´åˆã¯å‰Šé™¤ã¨ã—ã¦æ‰±ã†\n",
        "                try:\n",
        "                    nums = [int(n) for n in user_in.replace('ã€', ',').replace('ï¼Œ', ',').replace(' ', '').split(',') if n]\n",
        "                    actual_val = [w for i, w in enumerate(current_list, 1) if i not in nums]\n",
        "                    mode_msg = f\"ã€ç•ªå· {','.join(map(str, nums))} ã‚’å‰Šé™¤ã€‘\"\n",
        "                except:\n",
        "                    print(\"âš ï¸ å…¥åŠ›ãŒä¸æ­£ã§ã™ã€‚å†å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\")\n",
        "                    continue\n",
        "            else:\n",
        "                # ãã®ä»–ã¯å…¨ç½®æ›ã¨ã—ã¦æ‰±ã†\n",
        "                actual_val = [w.strip().lstrip('+').lstrip('ï¼‹') for w in user_in.replace('ã€', ',').replace('ï¼Œ', ',').split(',') if w.strip()]\n",
        "                mode_msg = \"ã€å…¥åŠ›å†…å®¹ã§ç½®æ›ã€‘\"\n",
        "\n",
        "        disp_val = \",\".join(actual_val) if actual_val else \"(ç©º)\"\n",
        "\n",
        "        print(f\"\\nğŸ“ {mode_msg}\")\n",
        "        print(f\"   ç¢ºå®šã•ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€ {disp_val} ã€‘\")\n",
        "        confirm = input(\"   ã“ã‚Œã§ç¢ºå®šã—ã¾ã™ã‹ï¼Ÿ (Enter = OK / n = ã‚„ã‚Šç›´ã—): \").strip().lower()\n",
        "        if confirm == '' or confirm == 'y':\n",
        "            return actual_val\n",
        "        else:\n",
        "            print(\"ğŸ”„ å†å…¥åŠ›ã—ã¾ã™...\\n\")\n",
        "\n",
        "def remove_duplicates(lst):\n",
        "    return list(dict.fromkeys(lst))\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå‡¦ç†\n",
        "# ---------------------------------------------------------\n",
        "æ™‚åˆ»è¡¨ç¤º('æ¤œç´¢èªé¸æŠã®é–‹å§‹')\n",
        "planner = StrategyPlanner()\n",
        "GLOBAL_SEARCH_QUEUE = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨ˆç”» (AIé™¤å¤–ææ¡ˆãƒ¢ãƒ¼ãƒ‰)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "temp_plans = []\n",
        "\n",
        "for categ in myclasses:\n",
        "    print(f\"\\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: ã€ {categ} ã€‘ ã®è¨­å®š\")\n",
        "\n",
        "    sub_cats, cat_en, is_human = planner.get_subcategories(categ)\n",
        "    default_keywords = sub_cats[:8]\n",
        "\n",
        "    print(f\"\\nğŸ‘‡ '{categ}' ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç¢ºå®šã—ã¦ãã ã•ã„\")\n",
        "    prompt_kwd = \"å…¥åŠ›ä¾‹:\"\n",
        "    base_keywords = safe_input_loop(prompt_kwd, default_keywords)\n",
        "\n",
        "    # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ææ¡ˆ\n",
        "    rec_negs = ['illustration', 'cartoon', 'toy', 'sketch', 'clipart']\n",
        "    if not planner._is_food_category(cat_en):\n",
        "        rec_negs.extend(['food', 'recipe'])\n",
        "\n",
        "    print(f\"\\nğŸ‘‡ '{categ}' ã®é™¤å¤–ãƒ¯ãƒ¼ãƒ‰(NOT)ã‚’ç¢ºå®šã—ã¦ãã ã•ã„\")\n",
        "    neg_words = safe_input_loop(\"AIãŒã‚«ãƒ†ã‚´ãƒªã«åˆã‚ã›ã¦æ¨å¥¨ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸ:\", rec_negs)\n",
        "    if isinstance(neg_words, str): neg_words = [neg_words]\n",
        "\n",
        "    final_queries = []\n",
        "    final_queries.extend(base_keywords)\n",
        "    for base in base_keywords:\n",
        "        for action in ACTIONS_AND_CONTEXTS:\n",
        "            final_queries.append(f\"{base} {action}\")\n",
        "    final_queries = remove_duplicates(final_queries)\n",
        "\n",
        "    temp_plans.append({\n",
        "        'categ': categ,\n",
        "        'queries': final_queries,\n",
        "        'neg': neg_words,\n",
        "        'is_human': is_human,\n",
        "        'cat_en': cat_en\n",
        "    })\n",
        "\n",
        "    print(f\"âœ… '{categ}' ã®è¨­å®šå®Œäº† (ã‚¯ã‚¨ãƒªæ•°: {len(final_queries)})\")\n",
        "\n",
        "for plan in temp_plans:\n",
        "    GLOBAL_SEARCH_QUEUE[plan['categ']] = {\n",
        "        'queries': plan['queries'],\n",
        "        'neg': plan['neg'],\n",
        "        'is_human': plan['is_human'],\n",
        "        'cat_en': plan['cat_en']\n",
        "    }\n",
        "\n",
        "print(\"\\nâœ¨ å…¨ã‚«ãƒ†ã‚´ãƒªã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸï¼ æ¬¡ã®ã‚»ãƒ«ã¸é€²ã‚“ã§ãã ã•ã„ã€‚\")\n",
        "æ™‚åˆ»è¡¨ç¤º('æ¤œç´¢èªæ±ºå®šå®Œäº†')"
      ],
      "metadata": {
        "id": "gtabOKXmNWih",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [4] è‡ªå‹•åé›†ã¨è‡ªå‹•é¸åˆ¥\n",
        "Bingæ¤œç´¢ã§æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ç”»åƒã‚’é›†ã‚ã€ç”»åƒã‚’èªè­˜ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆã£ã¦ã„ãªã„ã‚‚ã®ã‚„æ’é™¤å¯¾è±¡ãŒå†™ã£ã¦ã„ã‚‹ã‚‚ã®ã‚’å–ã‚Šé™¤ãã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "TtmzAZ_wNvWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [ï¼”] è‡ªå‹•åé›†ã¨è‡ªå‹•é¸åˆ¥ï¼ˆå¤‰æ•°æ´»ç”¨ãƒ»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç‰ˆï¼‰\n",
        "# ==============================================================================\n",
        "# 4. ãƒ•ã‚§ãƒ¼ã‚º2ï¼šå…¨è‡ªå‹•å®Ÿè¡Œï¼ˆåé›† ï¼‹ CLIPé¸åˆ¥ ï¼‹ å³æ™‚è»¢é€ï¼‰\n",
        "# ==============================================================================\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "\n",
        "æ™‚åˆ»è¡¨ç¤º('åé›†é–‹å§‹')\n",
        "\n",
        "if not GLOBAL_SEARCH_QUEUE:\n",
        "    print(\"âš ï¸ ã‚¨ãƒ©ãƒ¼: STEP 3 ãŒã¾ã å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸš€ ç”»åƒåé›†ã‚¹ã‚¿ãƒ¼ãƒˆ (ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰)\")\n",
        "    print(f\"   ä½œæ¥­å ´æ‰€: {TMPFOLDER}\")\n",
        "    print(f\"   ä¿å­˜å ´æ‰€: {FINAL_SAVE_DIR}\")\n",
        "    print(\"   1ã‚«ãƒ†ã‚´ãƒªçµ‚ã‚ã‚‹ã”ã¨ã«Google Driveã¸è»¢é€ã—ã¾ã™ã€‚\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    clip_filter = ImageFilterAI() if USE_CLIP_FILTER else None\n",
        "\n",
        "    # ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰ã®åˆæœŸåŒ–\n",
        "    if os.path.exists(TMPFOLDER):\n",
        "        shutil.rmtree(TMPFOLDER)\n",
        "\n",
        "    # è¿½åŠ ã®CLIPé™¤å¤–ãƒ¯ãƒ¼ãƒ‰\n",
        "    CLIP_EXTRA_NEGS = [\n",
        "        'text', 'watermark', 'logo', 'vector', 'low quality', 'blurry',\n",
        "        'screenshot', 'collage', 'frame', 'border', 'cgi', '3d render'\n",
        "    ]\n",
        "\n",
        "    for i, (categ, plan) in enumerate(GLOBAL_SEARCH_QUEUE.items()):\n",
        "        print(f\"\\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒª {i+1}/{len(GLOBAL_SEARCH_QUEUE)} å‡¦ç†ä¸­: ã€ {categ} ã€‘\")\n",
        "\n",
        "        queries = plan['queries']\n",
        "        neg_list = plan['neg']\n",
        "        is_human = plan['is_human']\n",
        "        neg_str = \" \".join([f\"-{n}\" for n in neg_list])\n",
        "\n",
        "        # ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¸€æ™‚ãƒ‘ã‚¹ (./tmp_images/ãƒã‚³ ãªã©)\n",
        "        local_cat_dir = os.path.join(TMPFOLDER, categ)\n",
        "        os.makedirs(local_cat_dir, exist_ok=True)\n",
        "\n",
        "        # ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼è¨­å®š (ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜)\n",
        "        crawler = BingImageCrawler(storage={\"root_dir\": local_cat_dir}, downloader_threads=10)\n",
        "\n",
        "        # --- 1. åé›† ---\n",
        "        query_index = 0\n",
        "        while True:\n",
        "            current_count = len(os.listdir(local_cat_dir))\n",
        "\n",
        "            if current_count >= target_num * 1.3:\n",
        "                print(f\"   ğŸ›‘ åé›†å®Œäº† (ç¾åœ¨: {current_count}æš)\")\n",
        "                break\n",
        "\n",
        "            if query_index >= len(queries):\n",
        "                print(\"   âš ï¸ ã‚¯ã‚¨ãƒªåˆ‡ã‚Œ\")\n",
        "                break\n",
        "\n",
        "            q_word = queries[query_index]\n",
        "            full_query = f\"{q_word} {categ} {neg_str}\"\n",
        "\n",
        "            if query_index % 5 == 0:\n",
        "                print(f\"      [{current_count}æš] ğŸ” {full_query}\")\n",
        "\n",
        "            try:\n",
        "                crawler.crawl(keyword=full_query, filters={'type': 'photo'}, max_num=50, overwrite=False, file_idx_offset='auto')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            query_index += 1\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        # --- 2. é¸åˆ¥ (CLIP + äººç‰©) ---\n",
        "        if clip_filter:\n",
        "            combined_negs = neg_list + CLIP_EXTRA_NEGS\n",
        "            clip_filter.filter(local_cat_dir, categ, combined_negs, CLIP_THRESHOLD, is_human_category=is_human)\n",
        "\n",
        "        final_count = len(os.listdir(local_cat_dir))\n",
        "        print(f\"   ğŸ‰ é¸åˆ¥å®Œäº†: {final_count} æš\")\n",
        "\n",
        "        # --- 3. Google Driveã¸å³æ™‚è»¢é€ ---\n",
        "        print(f\"   ğŸšš Google Driveã¸è»¢é€ä¸­... ({categ})\")\n",
        "\n",
        "        # æœ€çµ‚ä¿å­˜å…ˆã®ãƒ‘ã‚¹ (drive/MyDrive/tmp/ãƒã‚³ ãªã©)\n",
        "        drive_cat_dir = os.path.join(FINAL_SAVE_DIR, categ)\n",
        "\n",
        "        os.makedirs(drive_cat_dir, exist_ok=True)\n",
        "\n",
        "        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼\n",
        "        files_to_move = os.listdir(local_cat_dir)\n",
        "        for fname in files_to_move:\n",
        "            src = os.path.join(local_cat_dir, fname)\n",
        "            dst = os.path.join(drive_cat_dir, fname)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "        print(f\"   âœ… è»¢é€å®Œäº†ï¼ Driveã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {drive_cat_dir}\")\n",
        "        print(f\"   (æ¬¡ã®ã‚«ãƒ†ã‚´ãƒªã«é€²ã¿ã¾ã™...)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "æ™‚åˆ»è¡¨ç¤º('åé›†å®Œäº†')"
      ],
      "metadata": {
        "id": "Zj37vqMTNzQu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ERRORãŒå‡ºã‚‹ã®ã¯ãƒªãƒ³ã‚¯åˆ‡ã‚Œ(404)ã‚„ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™(403)ã§ã‚ã‚Šã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„ã®ã§æ°—ã«ã—ãªãã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚\n",
        "\n",
        "## 2-2 ç”»åƒã®é¸åˆ¥\n",
        "\n",
        "![Google Drive](https://raw.githubusercontent.com/aquapathos/pictures/gh-pages/BasicAIFig001.png)ã€€ã€€ã€€ã€€ã€€ã€€ ![Google Drive](https://raw.githubusercontent.com/aquapathos/pictures/gh-pages/BasicAIFig002.png)   \n",
        "\n",
        "![Google Drive](https://github.com/aquapathos/pictures/blob/gh-pages/BasicAIFig003.png?raw=true)\n",
        "\n",
        "1. <font color='blue'>ãƒ–ãƒ©ã‚¦ã‚¶ã§æ–°ã—ã„ã‚¿ãƒ–ã‚’ã¤ãã‚Šã€[ãƒ‰ãƒ©ã‚¤ãƒ–](https://drive.google.com/) ã‚’é–‹ãã¾ã™ã€‚\n",
        "2. **ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–**ã®ä¸­ã®**tmp**ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã„ã¦ã¿ã¦ãã ã•ã„ã€‚ è‡ªåˆ†ãŒæ±ºã‚ãŸã‚«ãƒ†ã‚´ãƒªã®ãƒ•ã‚©ãƒ«ãƒ€ãŒä¸¦ã‚“ã§ã„ã‚‹ã¯ãšã§ã™ã€‚\n",
        "3.ã€Œãƒã‚³ã€ã‚’ä¾‹ã¨ã—ã¦èª¬æ˜ã—ã¾ã™ã€‚**ãƒã‚³** ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ãã¾ã™ã€‚\n",
        "4. ãƒªã‚¹ãƒˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‹ã‚‰ã‚°ãƒªãƒƒãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã«åˆ‡ã‚Šæ›¿ãˆã¦ä¸é©ã ã¨æ€ã‚ã‚Œã‚‹ç”»åƒã‚’å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚\n",
        "5. åŒæ§˜ã«ã—ã¦ã€ã»ã‹ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹ã§ã¯ã‚¤ãƒŒã€ç¯å°ã€ã‚¹ã‚¯ãƒ¼ã‚¿ï¼‰ã«ã¤ã„ã¦ã‚‚ä¸è¦ç”»åƒã‚’å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚</font>\n",
        "\n",
        "ä¸é©ãªã®ã¯ã€æ¬¡ã®ã‚ˆã†ãªç”»åƒã§ã™ã€‚\n",
        "- ãƒã‚³ãŒå†™ã£ã¦ã„ãªã„ç”»åƒã€‚å†™ã£ã¦ã„ã¦ã‚‚å°ã•ã™ãã‚‹ç”»åƒ\n",
        "- ä¸»è¦è¢«å†™ä½“ãŒãƒã‚³ã§ã¯ãªã„ç”»åƒã€ã¤ã¾ã‚Šãƒã‚³ä»¥å¤–ã®ã‚‚ã®ãŒç›®ç«‹ã£ã¦ã„ã‚‹ç”»åƒ\n",
        "- æ–‡å­—ãŒç›®ç«‹ã¤ç”»åƒã€‚ã§ãã‚Œã°æ–‡å­—ã¯å…¥ã£ã¦ã„ãªã„ã“ã¨ãŒæœ›ã¾ã—ã„ã§ã™ãŒã€ç›®ç«‹ãŸãªã‘ã‚Œã°OK\n",
        "- ãƒ‡ãƒ•ã‚©ãƒ«ãƒ¡ã•ã‚ŒãŸã¬ã„ãã‚‹ã¿ã‚„ã‚¤ãƒ©ã‚¹ãƒˆã€‚\n",
        "- GIF å‹•ç”»\n",
        "- åŒä¸€ã®å†™çœŸ\n",
        "\n",
        "ãƒ‘ãƒƒã¨è¦‹ã¦ã€ã“ã‚Œã¯ã€Œãƒã‚³ã€ãŒä¸»é¡Œã®å†™çœŸã ã¨æ„Ÿã˜ã‚‰ã‚Œã‚‹ã‚‚ã®ã‚’æ®‹ã—ã¦ã€ãã†ã§ãªã„ã‚‚ã®ã€ä»–ã®ãƒ¢ãƒãŒå¤šãã®é¢ç©ã‚’å ã‚ã¦ã„ã‚‹ã‚‚ã®ã¯å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚(å³å¯†ãªãƒ«ãƒ¼ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚ãªãŸè‡ªèº«ãŒã“ã‚Œã¯ä½•ã‹ã¨å•ã‚ã‚Œã¦ã€Œãƒã‚³ã€ã¨ç­”ãˆã‚‹å†™çœŸã‚’é¸ã¹ã°ã„ã„ã§ã™ã€‚ã‚ãªãŸã®é¸ã‚“ã ã‚‚ã®ãŒæ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ãªã‚Šã¾ã™ã€‚)\n",
        "\n",
        "# 2-3ã€€ç”»åƒã®ãƒªã‚µã‚¤ã‚º\n",
        "\n",
        "ä½•åº¦ã‚‚ä½¿ã†ã®ã§é–¢æ•°ã‚’å®šç¾©ã—ã¦ãŠãã¾ã™ã€‚  <font color='blue'>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</font>ã€€ã€€ï¼ˆã‚³ãƒ¼ãƒ‰ã¯éè¡¨ç¤ºã«ãªã£ã¦ã„ã¾ã™ãŒã€å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ï¼‰"
      ],
      "metadata": {
        "id": "gqymBGpNbcPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ãƒªã‚µã‚¤ã‚ºé–¢æ•°ã®å®šç¾©\n",
        "def resize(foldername, size=ViewSIZE):\n",
        "    imgnames = glob.glob(foldername+\"/*\") # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ\n",
        "    images = []\n",
        "    for imgname in imgnames:\n",
        "        img = cv2.cvtColor(np.array(Image.open(imgname).convert('RGBA')),cv2.COLOR_RGBA2RGB)\n",
        "        height = img.shape[0]\n",
        "        width = img.shape[1]\n",
        "        if height > width :\n",
        "            m = (height - width)//2\n",
        "            img = img[m:m+width]\n",
        "        else:\n",
        "            m = (width - height)//2\n",
        "            img = img[:,m:m+height]\n",
        "        img = cv2.resize(img, (size,size))\n",
        "        images.append(img)\n",
        "        img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(imgname,img)\n",
        "    return images"
      ],
      "metadata": {
        "id": "2NBYmlB2b85L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã•ã£ãããƒªã‚µã‚¤ã‚ºã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã€€ã€€<font color='blue'>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</font>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jg41GoTmnqb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "input('ä¸é©åˆ‡ç”»åƒã®å‰Šé™¤ã¯çµ‚äº†ã—ãŸã‚‰ Enterã‚’æŠ¼ã—ã¦ãã ã•ã„')\n",
        "# ãƒªã‚µã‚¤ã‚ºã‚’å®Ÿè¡Œ\n",
        "images = []\n",
        "for i,categ in enumerate(myclasses):\n",
        "  images.append(resize(FINAL_SAVE_DIR+categ))\n",
        "  print(f\"{categ} - {len(images[i])} æšã®ç”»åƒãŒãƒªã‚µã‚¤ã‚ºã•ã‚Œã¾ã—ãŸ\")\n",
        "images = np.array(images, dtype=object) ## numpy ä»•æ§˜å¤‰æ›´ã‚ã‚Š 2024"
      ],
      "metadata": {
        "id": "nhRak6dVb8YB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¡¨ç¤ºç”¨é–¢æ•°ã‚’å®šç¾©ã—ã€è¡¨ç¤ºã—ã¦ç¢ºèªã—ã¦ã¿ã¾ã™ã€‚ã€€ã€€<font color='blue'>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</font>ã€€ã‚³ãƒ¼ãƒ‰ã¯éè¡¨ç¤ºã«ã—ã¦ã‚ã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "ZaC3zB64st95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title è¡¨ç¤ºç”¨é–¢æ•°ã®å®šç¾©\n",
        "import math\n",
        "# startç•ªã‹ã‚‰npicæšè¡¨ç¤ºã™ã‚‹é–¢æ•°ã‚’å®šç¾©\n",
        "plt.rcParams['figure.figsize'] = (12.0, 7.0)\n",
        "def showimg(images, start = 0, npic = 48):\n",
        "    n = npic if len(images) >= start+npic else len(images) - start\n",
        "    plt.figure(figsize=(8,7.5*(math.ceil(n/8))/6),dpi=150)\n",
        "    i = 0\n",
        "    while True:\n",
        "        if i < n :\n",
        "            plt.subplot((n-1)//8+1,8,i+1)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.imshow(images[start+i])\n",
        "            plt.title(\"{}\".format(start+i))\n",
        "            i += 1\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "id": "uHkausWAs0uq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã•ã£ããè¡¨ç¤ºã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã€€ã€€<font color='blue'>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</font>"
      ],
      "metadata": {
        "id": "BnOHc5C2s7pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "for i in range(len(images)):\n",
        "  showimg(images = images[i],start=0,npic=8) # å„ã‚«ãƒ†ã‚´ãƒª16æšãšã¤è¡¨ç¤º"
      ],
      "metadata": {
        "id": "ZEpFpOLBtCDb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-4 å­¦ç¿’ç”¨ç”»åƒãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿\n",
        "python ã«ã¯ãƒ¡ãƒ¢ãƒªä¸Šã«ç½®ã‹ã‚ŒãŸå¤‰æ•°ã®å€¤ã‚’ãã£ãã‚Šãã®ã¾ã¾ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹é–¢æ•°ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã®ã§ã€ãã®é–¢æ•°ã‚’ä½¿ã£ã¦å¤‰æ•°ã®ä¸­èº«ã‚’ Google ãƒ‰ãƒ©ã‚¤ãƒ–ã«ä¿å­˜ã—ãŸã‚Šã€èª­ã¿å‡ºã—ãŸã‚Šã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãŠãã¾ã™ã€‚ã€€ã€€<font color='blue'>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</font>\n"
      ],
      "metadata": {
        "id": "9Mbv_Nz0vAW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ pickle å½¢å¼ã§ä¿å­˜\n",
        "def storeCategoryImages(cat, fname, folder = \".\"):\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    f = open(folder+\"/\"+fname,'wb')\n",
        "    pickle.dump(cat,f)\n",
        "    f.close\n",
        "\n",
        "# pickle å½¢å¼ã§ä¿å­˜ã•ã‚ŒãŸç”»åƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "def loadCategoryImages(fname, folder = \".\"):\n",
        "    f = open(folder+\"/\"+fname,'rb')\n",
        "    cat = pickle.load(f)\n",
        "    f.close\n",
        "    return cat"
      ],
      "metadata": {
        "id": "c2D9gkvmvc2B",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã•ã£ãããƒªã‚µã‚¤ã‚ºã—ãŸç”»åƒã‚’pickle å½¢å¼ã§ä¿å­˜ã—ã¾ã—ã‚‡ã†ã€‚ã€€ã€€<font color='blue'>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</font>"
      ],
      "metadata": {
        "id": "mW9oB5LwvsF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GFOLDER = \"drive/MyDrive/CIFAR10\"  # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ç”¨ã®ãƒ•ã‚©ãƒ«ãƒ€å\n",
        "for i,categ in enumerate(myclasses):\n",
        "  print(i,categ)\n",
        "  storeCategoryImages(images[i],f\"{categ}.pkl\",folder=GFOLDER)"
      ],
      "metadata": {
        "id": "5LU-sIppv1lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»Šåº¦ã¯é€†ã« pickle å½¢å¼ã§ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ•°ã«èª­ã¿è¾¼ã¿ã€æ­£ã—ãå¾©å…ƒã§ãã‚‹ã‹ç¢ºèªã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚ä¸€ã¤ç›®ã®ã‚«ãƒ†ã‚´ãƒªã ã‘è©¦ã—ã¦ã¿ã¾ã™ã€‚ã€€ã€€<font color='blue'>æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚</font>"
      ],
      "metadata": {
        "id": "E5F_iuJ1wrzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testimg = loadCategoryImages(\"ãƒã‚³.pkl\", folder=GFOLDER)\n",
        "showimg(images = testimg,start = 4,npic=8)"
      ],
      "metadata": {
        "id": "xFr-rspjxImU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR10PyTorch003 ã«é€²ã‚“ã§ãã ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "iqW4ws4PaKFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "uz1Ht1bhaFtQ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
